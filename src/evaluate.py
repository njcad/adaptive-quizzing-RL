"""
Evaluate policy generated by q-learning. Compare to random action baseline.
Evaluation metric is expected reward.
"""

import pandas as pd
from tqdm import tqdm
import random 

random.seed(3)


def load_policy(policy_path):
    """
    Load the learned policy of state : action.
    Convert to a python dictionary for ease of use.
    """
    policy = {}
    with open(policy_path, 'r') as f:
        for line in f:
            line = line.strip()
            state, action = line.split(" : ")
            policy[state] = int(action)

    return policy


def build_transitions(sar_data):
    """
    Convert sar_data dataframe to a dict for ease of use.
    Returns dict {state: [(action, reward, next state)]}
    """
    # use groupby to group rows by state
    grouped = sar_data.groupby('s')

    # build transitions using dictionary comprehension
    transitions = {
        state: list(zip(group['a'], group['r'], group['sp'])) for state, group in grouped
    }

    return transitions


def evaluate_policy(transitions, policy=None, num_episodes=1000, gamma=0.99, max_steps=20):
    """
    Evaluate the policy by expected reward.
        transitions: dict of {state : [(action, reward, next state)]}
        policy: dict of learned {state: action}
    """
    # track rewards through episodes
    total_reward = 0

    for _ in tqdm(range(num_episodes)):
        current_state = random.choice(list(transitions.keys()))
        episode_reward = 0
        discount = 1.0
        episode_length = 0

        while episode_length < max_steps:
            # if terminal state, pick a new state to continue episode
            if current_state not in transitions:
                current_state = random.choice(list(transitions.keys()))

            # for learned policy version
            if policy:  
                # if state is in policy, find policy action and valid transitions to next states
                if current_state in policy:
                    action = policy[current_state]
                    valid_transitions = [(a, r, sp) for a, r, sp in transitions[current_state] if a == action]
                
                # if no valid transitions for policy best action, pick a new random state continually
                if not valid_transitions:
                    current_state = random.choice(list(transitions.keys()))
                    continue
            
            # for random baseline version
            else:
                 # if no valid transitions, pick a new random state continually
                if not transitions[current_state]:
                    current_state = random.choice(list(transitions.keys()))
                    continue
                valid_transitions = transitions[current_state]
                
            # select a valid transition at random, track reward and update state
            _, reward, next_state = random.choice(valid_transitions)
            episode_reward += reward * discount
            discount *= gamma
            current_state = next_state
            episode_length += 1

        # ensure episode was valid length and update total reward
        assert(episode_length == max_steps)
        total_reward += episode_reward

    return total_reward / num_episodes


def policy_details(policy):
    """
    Helper function to grab the distribution of question types recommended by the learned policy.
    """
    levels = {1: 0, 2: 0, 3: 0, 4: 0, 5: 0}
    for key in policy:
        levels[policy[key]] += 1

    for key in levels:
        print(f"Number of learned recommendations for question level {key}: {levels[key]}")
              
            
def main():
    """
    Main driver function to evaluate policy.
    """
    # load in the policy data
    policy_path = "../policies/v4.policy"
    policy = load_policy(policy_path)
    print("Policy loaded as dict...")

    # quick evaluation of distribution of learned recommendations
    policy_details(policy)

    # load in the state, action, reward, next state dataframe
    sar_data_path = "../sar_spaces/sar_space.csv"
    sar_data = pd.read_csv(sar_data_path)
    print("SAR data loaded...")

    # get transitions
    transitions = build_transitions(sar_data)
    print("Transitions calculated...")

    # calculate expected rewards
    num_episodes = 10000
    print("Calculating learned expected reward...")
    learned_expected_reward = evaluate_policy(transitions, policy, num_episodes=num_episodes)
    print("Calculating random expected reward...")
    random_expected_reward = evaluate_policy(transitions, num_episodes=num_episodes)

    # reward is given by average reward per episode, which are normalized to length 20 questions
    print(f"Expected reward from learned policy: {learned_expected_reward}")
    print(f"Expected reward from random policy: {random_expected_reward}")


if __name__ == '__main__':
    main()
